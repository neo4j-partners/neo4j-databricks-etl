{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load London Transport Network to Neo4j\n",
    "\n",
    "This notebook loads the **London Transport Network** dataset from CSV files into Neo4j via Delta Lake tables.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Loads CSVs** — Reads London stations and tube lines from Unity Catalog Volume\n",
    "2. **Creates Delta Tables** — Stores data in Delta Lake for validation and transformation\n",
    "3. **Creates Nodes** — Writes Station nodes to Neo4j\n",
    "4. **Creates Relationships** — Establishes tube line connections between stations\n",
    "5. **Validates Data** — Provides queries to verify successful loading\n",
    "\n",
    "## Data Overview\n",
    "\n",
    "- **302 London stations** with names, zones, postcodes, and coordinates\n",
    "- **Tube line connections** showing which lines connect which stations\n",
    "- **Bidirectional relationships** for each tube line\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. ✅ Neo4j database created (Aura or self-hosted)\n",
    "2. ✅ Databricks Secrets configured (`neo4j-creds` scope with `username` and `password`)\n",
    "3. ✅ Databricks cluster with Neo4j Connector library installed\n",
    "4. ✅ Unity Catalog Volume created with CSV files uploaded\n",
    "\n",
    "**Neo4j Connector Library:**\n",
    "- Maven coordinates: `org.neo4j:neo4j-connector-apache-spark_2.12:5.3.1_for_spark_3`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Configure Connection and Paths\n",
    "\n",
    "Use interactive widgets to configure Neo4j connection and Unity Catalog paths with sensible defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing widgets if any\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "# Neo4j connection widgets\n",
    "dbutils.widgets.text(\"neo4j_url\", \"bolt://localhost:7687\", \"Neo4j URL\")\n",
    "dbutils.widgets.text(\"neo4j_username\", \"neo4j\", \"Neo4j Username\")\n",
    "dbutils.widgets.text(\"neo4j_database\", \"neo4j\", \"Neo4j Database\")\n",
    "\n",
    "# Unity Catalog and Delta Lake widgets\n",
    "dbutils.widgets.text(\"catalog_name\", \"london_catalog\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"london_schema\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"volume_name\", \"london_transport\", \"Volume Name\")\n",
    "\n",
    "print(\"✓ Widgets created successfully\")\n",
    "print(\"\\nConfigure the widgets above with your specific values, then run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get widget values\n",
    "NEO4J_URL = dbutils.widgets.get(\"neo4j_url\")\n",
    "NEO4J_USER = dbutils.widgets.get(\"neo4j_username\")\n",
    "NEO4J_DB = dbutils.widgets.get(\"neo4j_database\")\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "VOLUME = dbutils.widgets.get(\"volume_name\")\n",
    "\n",
    "# Retrieve Neo4j password from Databricks Secrets\n",
    "# Note: Password is kept in secrets for security\n",
    "NEO4J_PASS = dbutils.secrets.get(scope=\"neo4j\", key=\"password\")\n",
    "\n",
    "# Configure Spark session for Neo4j\n",
    "spark.conf.set(\"neo4j.url\", NEO4J_URL)\n",
    "spark.conf.set(\"neo4j.authentication.basic.username\", NEO4J_USER)\n",
    "spark.conf.set(\"neo4j.authentication.basic.password\", NEO4J_PASS)\n",
    "spark.conf.set(\"neo4j.database\", NEO4J_DB)\n",
    "\n",
    "# Unity Catalog Volume path\n",
    "BASE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "\n",
    "# Delta Lake table names\n",
    "STATIONS_TABLE = f\"{CATALOG}.{SCHEMA}.london_stations\"\n",
    "TUBE_LINES_TABLE = f\"{CATALOG}.{SCHEMA}.london_tube_lines\"\n",
    "\n",
    "print(\"Configuration loaded from widgets:\")\n",
    "print(f\"✓ Neo4j URL: {NEO4J_URL}\")\n",
    "print(f\"✓ Neo4j User: {NEO4J_USER}\")\n",
    "print(f\"✓ Database: {NEO4J_DB}\")\n",
    "print(f\"✓ Catalog: {CATALOG}\")\n",
    "print(f\"✓ Schema: {SCHEMA}\")\n",
    "print(f\"✓ Volume: {VOLUME}\")\n",
    "print(f\"✓ Data path: {BASE_PATH}\")\n",
    "print(f\"✓ Stations table: {STATIONS_TABLE}\")\n",
    "print(f\"✓ Tube lines table: {TUBE_LINES_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Test Neo4j Connection\n",
    "\n",
    "Verify connectivity before proceeding with data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection by attempting to read from Neo4j\n",
    "try:\n",
    "    test_df = (\n",
    "        spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", NEO4J_URL)\n",
    "        .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "        .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "        .option(\"labels\", \"Station\")\n",
    "        .load()\n",
    "        .limit(5)\n",
    "    )\n",
    "    \n",
    "    count = test_df.count()\n",
    "    print(f\"✓ Connection successful! Found {count} Station nodes.\")\n",
    "    if count > 0:\n",
    "        print(\"\\nSample data:\")\n",
    "        display(test_df)\n",
    "    else:\n",
    "        print(\"\\n(No data loaded yet - this is expected on first run)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection failed: {str(e)}\")\n",
    "    print(\"\\nPlease verify:\")\n",
    "    print(\"  1. Neo4j database is running\")\n",
    "    print(\"  2. Connection URL is correct\")\n",
    "    print(\"  3. Credentials are valid\")\n",
    "    print(\"  4. Network/firewall allows connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Load CSVs to Delta Lake Tables\n",
    "\n",
    "Read CSV files from Unity Catalog Volume and write to Delta Lake tables for intermediate storage and validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify CSV Files\n",
    "\n",
    "Check that CSV files are accessible in the Unity Catalog Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List CSV files in the volume\n",
    "try:\n",
    "    files = dbutils.fs.ls(BASE_PATH)\n",
    "    print(f\"Files in {BASE_PATH}:\")\n",
    "    for file in sorted(files):\n",
    "        print(f\"  - {file.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error listing files: {str(e)}\")\n",
    "    print(f\"\\nPlease verify:\")\n",
    "    print(f\"  1. Volume path is correct: {BASE_PATH}\")\n",
    "    print(f\"  2. CSV files have been uploaded to the volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Stations CSV\n",
    "\n",
    "Read London_stations.csv and preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read stations CSV with header\n",
    "stations_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{BASE_PATH}/London_stations.csv\")\n",
    ")\n",
    "\n",
    "print(f\"Stations loaded: {stations_df.count()} rows\")\n",
    "print(\"\\nSchema:\")\n",
    "stations_df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "stations_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Transform Stations Data\n",
    "\n",
    "Select and rename columns to match Neo4j schema, and ensure proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns and rename for Neo4j\n",
    "stations_clean = (\n",
    "    stations_df\n",
    "    .select(\n",
    "        F.col(\"ID\").cast(\"integer\").alias(\"station_id\"),\n",
    "        F.col(\"Station_Name\").alias(\"name\"),\n",
    "        F.col(\"Latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "        F.col(\"Longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "        F.col(\"Zone\").alias(\"zone\"),\n",
    "        F.col(\"Postcode\").alias(\"postcode\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Transformed schema:\")\n",
    "stations_clean.printSchema()\n",
    "print(\"\\nSample transformed data:\")\n",
    "stations_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write Stations to Delta Lake Table\n",
    "\n",
    "Create Delta Lake table for stations data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta Lake table\n",
    "print(f\"Writing to Delta table: {STATIONS_TABLE}\")\n",
    "(\n",
    "    stations_clean\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(STATIONS_TABLE)\n",
    ")\n",
    "\n",
    "# Verify table creation\n",
    "stations_delta = spark.table(STATIONS_TABLE)\n",
    "print(f\"\\n✓ Delta table created: {stations_delta.count()} rows\")\n",
    "print(\"\\nSample data from Delta table:\")\n",
    "stations_delta.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Tube Lines CSV\n",
    "\n",
    "Read London_tube_lines.csv and preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tube lines CSV with header\n",
    "tube_lines_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(f\"{BASE_PATH}/London_tube_lines.csv\")\n",
    ")\n",
    "\n",
    "print(f\"Tube line connections loaded: {tube_lines_df.count()} rows\")\n",
    "print(\"\\nSchema:\")\n",
    "tube_lines_df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "tube_lines_df.show(10, truncate=False)\n",
    "\n",
    "# Show distinct tube lines\n",
    "print(\"\\nDistinct tube lines:\")\n",
    "tube_lines_df.select(\"Tube_Line\").distinct().orderBy(\"Tube_Line\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Write Tube Lines to Delta Lake Table\n",
    "\n",
    "Create Delta Lake table for tube lines data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta Lake table\n",
    "print(f\"Writing to Delta table: {TUBE_LINES_TABLE}\")\n",
    "(\n",
    "    tube_lines_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(TUBE_LINES_TABLE)\n",
    ")\n",
    "\n",
    "# Verify table creation\n",
    "tube_lines_delta = spark.table(TUBE_LINES_TABLE)\n",
    "print(f\"\\n✓ Delta table created: {tube_lines_delta.count()} rows\")\n",
    "print(\"\\nSample data from Delta table:\")\n",
    "tube_lines_delta.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Load Stations to Neo4j\n",
    "\n",
    "Write Station nodes from Delta Lake to Neo4j graph database.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Write Station Nodes to Neo4j\n",
    "\n",
    "Load stations from Delta table and write to Neo4j as Station nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"WRITING STATION NODES TO NEO4J\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Read from Delta table\n",
    "stations = spark.table(STATIONS_TABLE)\n",
    "\n",
    "print(f\"\\nWriting {stations.count()} Station nodes to Neo4j...\")\n",
    "\n",
    "# Write to Neo4j\n",
    "(\n",
    "    stations\n",
    "    .write\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    .mode(\"Overwrite\")\n",
    "    .option(\"labels\", \":Station\")\n",
    "    .option(\"node.keys\", \"station_id\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Station nodes written successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Index on Station ID\n",
    "\n",
    "Create index for performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index on station_id for better query performance\n",
    "index_query = \"CREATE INDEX station_id IF NOT EXISTS FOR (s:Station) ON (s.station_id)\"\n",
    "\n",
    "print(\"Creating index on station_id...\")\n",
    "try:\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "        .option(\"url\", NEO4J_URL) \\\n",
    "        .option(\"authentication.basic.username\", NEO4J_USER) \\\n",
    "        .option(\"authentication.basic.password\", NEO4J_PASS) \\\n",
    "        .option(\"database\", NEO4J_DB) \\\n",
    "        .option(\"query\", index_query) \\\n",
    "        .load() \\\n",
    "        .collect()\n",
    "    print(\"✓ Index created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Index creation result: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Verify Station Nodes\n",
    "\n",
    "Query Neo4j to confirm stations were loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count stations in Neo4j\n",
    "count_query = \"MATCH (s:Station) RETURN count(s) as station_count\"\n",
    "\n",
    "station_count_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", count_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"Station count in Neo4j:\")\n",
    "station_count_df.show()\n",
    "\n",
    "# Sample stations from Neo4j\n",
    "sample_query = \"MATCH (s:Station) RETURN s.station_id, s.name, s.zone, s.latitude, s.longitude LIMIT 5\"\n",
    "\n",
    "sample_stations_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", sample_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"\\nSample stations from Neo4j:\")\n",
    "sample_stations_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Create Tube Line Relationships\n",
    "\n",
    "Create bidirectional relationships between stations for each tube line.\n",
    "\n",
    "**Note:** Starting with Bakerloo line as proof-of-concept. Extend to all lines after validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Prepare Relationship Data\n",
    "\n",
    "Filter tube lines data for relationship creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tube lines from Delta table\n",
    "tube_lines = spark.table(TUBE_LINES_TABLE)\n",
    "\n",
    "# Filter to Bakerloo line for proof-of-concept\n",
    "bakerloo_lines = tube_lines.filter(F.col(\"Tube_Line\") == \"Bakerloo\")\n",
    "\n",
    "print(f\"Bakerloo line connections: {bakerloo_lines.count()}\")\n",
    "print(\"\\nSample connections:\")\n",
    "bakerloo_lines.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create Relationships Using Custom Cypher\n",
    "\n",
    "Use custom Cypher query to create bidirectional relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CREATING BAKERLOO LINE RELATIONSHIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Custom Cypher query for creating bidirectional relationships\n",
    "create_rel_query = \"\"\"\n",
    "UNWIND $rows AS row\n",
    "MATCH (from:Station {name: row.From_Station})\n",
    "MATCH (to:Station {name: row.To_Station})\n",
    "MERGE (from)-[:BAKERLOO]->(to)\n",
    "MERGE (to)-[:BAKERLOO]->(from)\n",
    "\"\"\"\n",
    "\n",
    "# Collect data for batch processing\n",
    "bakerloo_data = bakerloo_lines.select(\"From_Station\", \"To_Station\").collect()\n",
    "rows_data = [{\"From_Station\": row.From_Station, \"To_Station\": row.To_Station} for row in bakerloo_data]\n",
    "\n",
    "print(f\"\\nCreating relationships for {len(rows_data)} connections...\")\n",
    "\n",
    "# Execute via Neo4j connector\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"From_Station\", StringType(), True),\n",
    "    StructField(\"To_Station\", StringType(), True)\n",
    "])\n",
    "\n",
    "rel_df = spark.createDataFrame(rows_data, schema)\n",
    "\n",
    "(\n",
    "    rel_df\n",
    "    .write\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    .mode(\"Append\")\n",
    "    .option(\"query\", create_rel_query)\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Bakerloo line relationships created successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Verify Relationships\n",
    "\n",
    "Query Neo4j to confirm relationships were created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Bakerloo relationships\n",
    "rel_count_query = \"MATCH ()-[r:BAKERLOO]->() RETURN count(r) as bakerloo_count\"\n",
    "\n",
    "rel_count_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", rel_count_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"Bakerloo relationship count:\")\n",
    "rel_count_df.show()\n",
    "\n",
    "# Sample relationship paths\n",
    "path_query = \"\"\"\n",
    "MATCH (from:Station)-[:BAKERLOO]->(to:Station)\n",
    "RETURN from.name as from_station, to.name as to_station\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "path_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", path_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"\\nSample Bakerloo line connections:\")\n",
    "path_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Validation Queries\n",
    "\n",
    "Additional validation queries to verify the complete graph.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation\n",
    "\n",
    "Run comprehensive validation queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall graph statistics\n",
    "stats_query = \"\"\"\n",
    "MATCH (s:Station)\n",
    "OPTIONAL MATCH ()-[r:BAKERLOO]->()\n",
    "RETURN \n",
    "  count(DISTINCT s) as total_stations,\n",
    "  count(DISTINCT r) as total_bakerloo_relationships\n",
    "\"\"\"\n",
    "\n",
    "stats_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", stats_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"\\nGraph Statistics:\")\n",
    "stats_df.show()\n",
    "\n",
    "# Stations with most connections\n",
    "connections_query = \"\"\"\n",
    "MATCH (s:Station)-[:BAKERLOO]-()\n",
    "RETURN s.name as station, count(*) as connections\n",
    "ORDER BY connections DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "connections_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", connections_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"\\nStations with Most Bakerloo Connections:\")\n",
    "connections_df.show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ VALIDATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To extend this implementation:\n",
    "\n",
    "1. **Add more tube lines** - Modify Step 10 to process all tube lines instead of just Bakerloo\n",
    "2. **Create dynamic relationship types** - Use the tube line name to create different relationship types (e.g., :CENTRAL, :DISTRICT)\n",
    "3. **Add relationship properties** - Include additional metadata on relationships\n",
    "4. **Optimize performance** - Batch process large relationship sets\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Connection errors:**\n",
    "- Verify Neo4j is running\n",
    "- Check firewall/network settings\n",
    "- Confirm credentials in Databricks Secrets\n",
    "\n",
    "**File not found errors:**\n",
    "- Verify Unity Catalog volume path\n",
    "- Confirm CSV files are uploaded\n",
    "- Check catalog/schema names\n",
    "\n",
    "**Relationship creation issues:**\n",
    "- Ensure Station nodes exist before creating relationships\n",
    "- Verify station names match exactly between CSV files\n",
    "- Check for case sensitivity in station names\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
