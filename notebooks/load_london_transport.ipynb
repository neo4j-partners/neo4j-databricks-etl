{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load London Transport Network to Neo4j\n",
    "\n",
    "This notebook loads the **complete London Transport Network** dataset from CSV files into Neo4j via Delta Lake tables.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Loads CSVs** — Reads London stations and tube lines from Unity Catalog Volume\n",
    "2. **Creates Delta Tables** — Stores data in Delta Lake for validation and transformation\n",
    "3. **Creates Nodes** — Writes Station nodes to Neo4j\n",
    "4. **Creates Relationships** — Establishes tube line connections with dynamic relationship types for each line\n",
    "5. **Validates Data** — Provides comprehensive queries to verify successful loading\n",
    "\n",
    "## Data Overview\n",
    "\n",
    "- **302 London stations** with names, zones, postcodes, and coordinates\n",
    "- **All tube line connections** with bidirectional relationships\n",
    "- **Dynamic relationship types** for each tube line (e.g., :BAKERLOO, :CENTRAL, :CIRCLE, :DISTRICT, etc.)\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "- **Part 1-2:** Load stations to Delta Lake and Neo4j\n",
    "- **Part 3-4:** Load Bakerloo line as proof-of-concept and validate\n",
    "- **Part 5-6:** Load all remaining tube lines with dynamic relationships and final validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. Neo4j database created (Aura or self-hosted)\n",
    "2. Databricks Secrets configured (`neo4j` scope with `password` key)\n",
    "3. Databricks cluster configured with:\n",
    "   - **Access mode**: Dedicated (formerly: Single user) - REQUIRED\n",
    "   - **Runtime**: 13.3 LTS or higher (Spark 3.x)\n",
    "   - **Neo4j Spark Connector** library installed (Maven)\n",
    "      - **Maven**: `org.neo4j:neo4j-connector-apache-spark_2.12:5.3.1_for_spark_3`\n",
    "   - **Neo4j Python Driver** library installed (PyPI)\n",
    "      - **PyPI**: `neo4j==6.0.2`\n",
    "   - **Important:** The Neo4j Spark Connector requires \"Dedicated\" access mode and will NOT work in Shared mode.\n",
    "4. Unity Catalog Volume created with CSV files uploaded\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Configure Connection and Paths\n",
    "\n",
    "Use interactive widgets to configure Neo4j connection and Unity Catalog paths with sensible defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing widgets if any\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "# Neo4j connection widgets\n",
    "dbutils.widgets.text(\"neo4j_url\", \"bolt://localhost:7687\", \"Neo4j URL\")\n",
    "dbutils.widgets.text(\"neo4j_username\", \"neo4j\", \"Neo4j Username\")\n",
    "dbutils.widgets.text(\"neo4j_database\", \"neo4j\", \"Neo4j Database\")\n",
    "\n",
    "# Unity Catalog and Delta Lake widgets\n",
    "dbutils.widgets.text(\"catalog_name\", \"london_catalog\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"london_schema\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"volume_name\", \"london_transport\", \"Volume Name\")\n",
    "\n",
    "print(\"✓ Widgets created successfully\")\n",
    "print(\"\\nConfigure the widgets above with your specific values, then run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get widget values\n",
    "NEO4J_URL = dbutils.widgets.get(\"neo4j_url\")\n",
    "NEO4J_USER = dbutils.widgets.get(\"neo4j_username\")\n",
    "NEO4J_DB = dbutils.widgets.get(\"neo4j_database\")\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "VOLUME = dbutils.widgets.get(\"volume_name\")\n",
    "\n",
    "# Retrieve Neo4j password from Databricks Secrets\n",
    "# Note: Password is kept in secrets for security\n",
    "NEO4J_PASS = dbutils.secrets.get(scope=\"neo4j\", key=\"password\")\n",
    "\n",
    "# Configure Spark session for Neo4j\n",
    "spark.conf.set(\"neo4j.url\", NEO4J_URL)\n",
    "spark.conf.set(\"neo4j.authentication.basic.username\", NEO4J_USER)\n",
    "spark.conf.set(\"neo4j.authentication.basic.password\", NEO4J_PASS)\n",
    "spark.conf.set(\"neo4j.database\", NEO4J_DB)\n",
    "\n",
    "# Unity Catalog Volume path\n",
    "BASE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "\n",
    "# Delta Lake table names\n",
    "STATIONS_TABLE = f\"{CATALOG}.{SCHEMA}.london_stations\"\n",
    "TUBE_LINES_TABLE = f\"{CATALOG}.{SCHEMA}.london_tube_lines\"\n",
    "\n",
    "print(\"Configuration loaded from widgets:\")\n",
    "print(f\"✓ Neo4j URL: {NEO4J_URL}\")\n",
    "print(f\"✓ Neo4j User: {NEO4J_USER}\")\n",
    "print(f\"✓ Database: {NEO4J_DB}\")\n",
    "print(f\"✓ Catalog: {CATALOG}\")\n",
    "print(f\"✓ Schema: {SCHEMA}\")\n",
    "print(f\"✓ Volume: {VOLUME}\")\n",
    "print(f\"✓ Data path: {BASE_PATH}\")\n",
    "print(f\"✓ Stations table: {STATIONS_TABLE}\")\n",
    "print(f\"✓ Tube lines table: {TUBE_LINES_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Test Neo4j Connection\n",
    "\n",
    "Verify connectivity before proceeding with data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection by attempting to read from Neo4j\n",
    "try:\n",
    "    test_df = (\n",
    "        spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", NEO4J_URL)\n",
    "        .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "        .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "        .option(\"labels\", \"Station\")\n",
    "        .load()\n",
    "        .limit(5)\n",
    "    )\n",
    "    \n",
    "    count = test_df.count()\n",
    "    print(f\"✓ Connection successful! Found {count} Station nodes.\")\n",
    "    if count > 0:\n",
    "        print(\"\\nSample data:\")\n",
    "        display(test_df)\n",
    "    else:\n",
    "        print(\"\\n(No data loaded yet - this is expected on first run)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection failed: {str(e)}\")\n",
    "    print(\"\\nPlease verify:\")\n",
    "    print(\"  1. Neo4j database is running\")\n",
    "    print(\"  2. Connection URL is correct\")\n",
    "    print(\"  3. Credentials are valid\")\n",
    "    print(\"  4. Network/firewall allows connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Load CSVs to Delta Lake Tables\n",
    "\n",
    "Read CSV files from Unity Catalog Volume and write to Delta Lake tables for intermediate storage and validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify CSV Files\n",
    "\n",
    "Check that CSV files are accessible in the Unity Catalog Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List CSV files in the volume\n",
    "try:\n",
    "    files = dbutils.fs.ls(BASE_PATH)\n",
    "    print(f\"Files in {BASE_PATH}:\")\n",
    "    for file in sorted(files):\n",
    "        print(f\"  - {file.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error listing files: {str(e)}\")\n",
    "    print(f\"\\nPlease verify:\")\n",
    "    print(f\"  1. Volume path is correct: {BASE_PATH}\")\n",
    "    print(f\"  2. CSV files have been uploaded to the volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Stations CSV\n",
    "\n",
    "Read London_stations.csv and preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read stations CSV with header\n",
    "stations_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{BASE_PATH}/London_stations.csv\")\n",
    ")\n",
    "\n",
    "print(f\"Stations loaded: {stations_df.count()} rows\")\n",
    "print(\"\\nSchema:\")\n",
    "stations_df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "stations_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Transform Stations Data\n",
    "\n",
    "Select and rename columns to match Neo4j schema, and ensure proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns and rename for Neo4j\n",
    "stations_clean = (\n",
    "    stations_df\n",
    "    .select(\n",
    "        F.col(\"ID\").cast(\"integer\").alias(\"station_id\"),\n",
    "        F.col(\"Station_Name\").alias(\"name\"),\n",
    "        F.col(\"Latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "        F.col(\"Longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "        F.col(\"Zone\").alias(\"zone\"),\n",
    "        F.col(\"Postcode\").alias(\"postcode\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Transformed schema:\")\n",
    "stations_clean.printSchema()\n",
    "print(\"\\nSample transformed data:\")\n",
    "stations_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write Stations to Delta Lake Table\n",
    "\n",
    "Create Delta Lake table for stations data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta Lake table\n",
    "print(f\"Writing to Delta table: {STATIONS_TABLE}\")\n",
    "(\n",
    "    stations_clean\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(STATIONS_TABLE)\n",
    ")\n",
    "\n",
    "# Verify table creation\n",
    "stations_delta = spark.table(STATIONS_TABLE)\n",
    "print(f\"\\n✓ Delta table created: {stations_delta.count()} rows\")\n",
    "print(\"\\nSample data from Delta table:\")\n",
    "stations_delta.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Tube Lines CSV\n",
    "\n",
    "Read London_tube_lines.csv and preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tube lines CSV with header\n",
    "tube_lines_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(f\"{BASE_PATH}/London_tube_lines.csv\")\n",
    ")\n",
    "\n",
    "print(f\"Tube line connections loaded: {tube_lines_df.count()} rows\")\n",
    "print(\"\\nSchema:\")\n",
    "tube_lines_df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "tube_lines_df.show(10, truncate=False)\n",
    "\n",
    "# Show distinct tube lines\n",
    "print(\"\\nDistinct tube lines:\")\n",
    "tube_lines_df.select(\"Tube_Line\").distinct().orderBy(\"Tube_Line\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Write Tube Lines to Delta Lake Table\n",
    "\n",
    "Create Delta Lake table for tube lines data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta Lake table\n",
    "print(f\"Writing to Delta table: {TUBE_LINES_TABLE}\")\n",
    "(\n",
    "    tube_lines_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(TUBE_LINES_TABLE)\n",
    ")\n",
    "\n",
    "# Verify table creation\n",
    "tube_lines_delta = spark.table(TUBE_LINES_TABLE)\n",
    "print(f\"\\n✓ Delta table created: {tube_lines_delta.count()} rows\")\n",
    "print(\"\\nSample data from Delta table:\")\n",
    "tube_lines_delta.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Load Stations to Neo4j\n",
    "\n",
    "Write Station nodes from Delta Lake to Neo4j graph database.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Write Station Nodes to Neo4j\n",
    "\n",
    "Load stations from Delta table and write to Neo4j as Station nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"WRITING STATION NODES TO NEO4J\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Read from Delta table\n",
    "stations = spark.table(STATIONS_TABLE)\n",
    "\n",
    "print(f\"\\nWriting {stations.count()} Station nodes to Neo4j...\")\n",
    "\n",
    "# Write to Neo4j\n",
    "(\n",
    "    stations\n",
    "    .write\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    .mode(\"Overwrite\")\n",
    "    .option(\"labels\", \":Station\")\n",
    "    .option(\"node.keys\", \"station_id\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Station nodes written successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Index on Station ID\n",
    "\n",
    "Create index for performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index on station_id for better query performance\n",
    "index_query = \"CREATE INDEX station_id IF NOT EXISTS FOR (s:Station) ON (s.station_id)\"\n",
    "\n",
    "print(\"Creating index on station_id...\")\n",
    "try:\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "        .option(\"url\", NEO4J_URL) \\\n",
    "        .option(\"authentication.basic.username\", NEO4J_USER) \\\n",
    "        .option(\"authentication.basic.password\", NEO4J_PASS) \\\n",
    "        .option(\"database\", NEO4J_DB) \\\n",
    "        .option(\"query\", index_query) \\\n",
    "        .load() \\\n",
    "        .collect()\n",
    "    print(\"✓ Index created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Index creation result: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Verify Station Nodes\n",
    "\n",
    "Query Neo4j to confirm stations were loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count stations in Neo4j\n",
    "count_query = \"MATCH (s:Station) RETURN count(s) as station_count\"\n",
    "\n",
    "station_count_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", count_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"Station count in Neo4j:\")\n",
    "station_count_df.show()\n",
    "\n",
    "# Sample stations from Neo4j\n",
    "sample_query = \"MATCH (s:Station) RETURN s.station_id AS station_id, s.name AS name, s.zone AS zone, s.latitude AS latitude, s.longitude AS longitude\"\n",
    "\n",
    "sample_stations_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", sample_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"\\nSample stations from Neo4j:\")\n",
    "sample_stations_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Create Tube Line Relationships\n",
    "\n",
    "Create bidirectional relationships between stations for each tube line.\n",
    "\n",
    "**Note:** Starting with Bakerloo line as proof-of-concept. Extend to all lines after validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Prepare Relationship Data\n",
    "\n",
    "Filter tube lines data for relationship creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tube lines from Delta table\n",
    "tube_lines = spark.table(TUBE_LINES_TABLE)\n",
    "\n",
    "# Filter to Bakerloo line for proof-of-concept\n",
    "bakerloo_lines = tube_lines.filter(F.col(\"Tube_Line\") == \"Bakerloo\")\n",
    "\n",
    "print(f\"Bakerloo line connections: {bakerloo_lines.count()}\")\n",
    "print(\"\\nSample connections:\")\n",
    "bakerloo_lines.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create Relationships Using Custom Cypher\n",
    "\n",
    "Use custom Cypher query to create bidirectional relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CREATING BAKERLOO LINE RELATIONSHIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for relationship creation\n",
    "bakerloo_data = bakerloo_lines.select(\n",
    "    F.col(\"From_Station\").alias(\"from_station\"),\n",
    "    F.col(\"To_Station\").alias(\"to_station\")\n",
    ")\n",
    "\n",
    "print(f\"\\nCreating relationships for {bakerloo_data.count()} connections...\")\n",
    "\n",
    "# Write relationships using Neo4j Spark Connector\n",
    "# The connector will create relationships for each row in the DataFrame\n",
    "(\n",
    "    bakerloo_data\n",
    "    .write\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    .mode(\"Append\")\n",
    "    .option(\"relationship\", \"BAKERLOO\")\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\n",
    "    .option(\"relationship.source.labels\", \":Station\")\n",
    "    .option(\"relationship.source.save.mode\", \"Match\")\n",
    "    .option(\"relationship.source.node.keys\", \"from_station:name\")\n",
    "    .option(\"relationship.target.labels\", \":Station\")\n",
    "    .option(\"relationship.target.save.mode\", \"Match\")\n",
    "    .option(\"relationship.target.node.keys\", \"to_station:name\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "# Create reverse relationships for bidirectional connections\n",
    "(\n",
    "    bakerloo_data\n",
    "    .select(\n",
    "        F.col(\"to_station\").alias(\"from_station\"),\n",
    "        F.col(\"from_station\").alias(\"to_station\")\n",
    "    )\n",
    "    .write\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    .mode(\"Append\")\n",
    "    .option(\"relationship\", \"BAKERLOO\")\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\n",
    "    .option(\"relationship.source.labels\", \":Station\")\n",
    "    .option(\"relationship.source.save.mode\", \"Match\")\n",
    "    .option(\"relationship.source.node.keys\", \"from_station:name\")\n",
    "    .option(\"relationship.target.labels\", \":Station\")\n",
    "    .option(\"relationship.target.save.mode\", \"Match\")\n",
    "    .option(\"relationship.target.node.keys\", \"to_station:name\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Bakerloo line relationships created successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Verify Relationships\n",
    "\n",
    "Query Neo4j to confirm relationships were created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Bakerloo relationships\n",
    "rel_count_query = \"MATCH ()-[r:BAKERLOO]->() RETURN count(r) as bakerloo_count\"\n",
    "\n",
    "rel_count_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", rel_count_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"Bakerloo relationship count:\")\n",
    "rel_count_df.show()\n",
    "\n",
    "# Sample relationship paths\n",
    "path_query = \"\"\"\n",
    "MATCH (from:Station)-[:BAKERLOO]->(to:Station)\n",
    "RETURN from.name as from_station, to.name as to_station\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "path_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", path_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"\\nSample Bakerloo line connections:\")\n",
    "path_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Validation of Bakerloo Line Proof-of-Concept\n",
    "\n",
    "Validate the Bakerloo line implementation before processing all tube lines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bakerloo Line Validation\n",
    "\n",
    "Verify the Bakerloo line was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BAKERLOO LINE VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall graph statistics\n",
    "# FIXED: Using modern COUNT{} subqueries instead of OPTIONAL MATCH to avoid Cartesian product\n",
    "# The previous query had uncorrelated patterns which created a cross product (every station × every relationship)\n",
    "# COUNT{} subqueries are more efficient and clearer in intent - each subquery runs independently\n",
    "stats_query = \"\"\"\n",
    "RETURN\n",
    "  count{MATCH (s:Station)} as total_stations,\n",
    "  count{MATCH ()-[r:BAKERLOO]->()} as total_bakerloo_relationships\n",
    "\"\"\"\n",
    "\n",
    "stats_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", stats_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"\\nGraph Statistics:\")\n",
    "stats_df.show()\n",
    "\n",
    "# Stations with most connections\n",
    "# IMPROVED: Using modern COUNT{} subquery syntax for explicit, readable counting\n",
    "# This makes it clear we're counting the pattern (s)-[:BAKERLOO]-() for each station\n",
    "# Also supports future optimization by Neo4j query planner\n",
    "connections_query = \"\"\"\n",
    "MATCH (s:Station)\n",
    "WHERE count{(s)-[:BAKERLOO]-()} > 0\n",
    "RETURN s.name as station, count{(s)-[:BAKERLOO]-()} as connections\n",
    "ORDER BY connections DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "connections_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "    .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "    .option(\"query\", connections_query)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"\\nStations with Most Bakerloo Connections:\")\n",
    "connections_df.show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ BAKERLOO LINE VALIDATED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Load All Tube Lines\n",
    "\n",
    "Now that Bakerloo line is validated, process all remaining tube lines with dynamic relationship types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Get All Tube Lines\n",
    "\n",
    "Retrieve all unique tube lines for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique tube lines (excluding Bakerloo since it's already done)\n",
    "tube_lines_list = (\n",
    "    tube_lines\n",
    "    .select(\"Tube_Line\")\n",
    "    .distinct()\n",
    "    .filter(F.col(\"Tube_Line\") != \"Bakerloo\")  # Exclude Bakerloo - already loaded\n",
    "    .rdd\n",
    "    .flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"Found {len(tube_lines_list)} remaining tube lines to process:\")\n",
    "for line in sorted(tube_lines_list):\n",
    "    print(f\"  - {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Create Dynamic Relationships for All Tube Lines\n",
    "\n",
    "Process each tube line and create relationships with dynamic relationship types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CREATING RELATIONSHIPS FOR ALL TUBE LINES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Process each tube line\n",
    "for line in sorted(tube_lines_list):\n",
    "    print(f\"\\nProcessing {line} line...\")\n",
    "    \n",
    "    # Filter data for this line and prepare columns\n",
    "    line_data = (\n",
    "        tube_lines\n",
    "        .filter(F.col(\"Tube_Line\") == line)\n",
    "        .select(\n",
    "            F.col(\"From_Station\").alias(\"from_station\"),\n",
    "            F.col(\"To_Station\").alias(\"to_station\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    connection_count = line_data.count()\n",
    "    \n",
    "    # Create relationship type (sanitize line name for Neo4j)\n",
    "    # Replace spaces and special characters with underscores\n",
    "    rel_type = line.upper().replace(\" \", \"_\").replace(\"&\", \"AND\")\n",
    "    \n",
    "    # Write relationships using Neo4j Spark Connector\n",
    "    (\n",
    "        line_data\n",
    "        .write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .mode(\"Append\")\n",
    "        .option(\"relationship\", rel_type)\n",
    "        .option(\"relationship.save.strategy\", \"keys\")\n",
    "        .option(\"relationship.source.labels\", \":Station\")\n",
    "        .option(\"relationship.source.save.mode\", \"Match\")\n",
    "        .option(\"relationship.source.node.keys\", \"from_station:name\")\n",
    "        .option(\"relationship.target.labels\", \":Station\")\n",
    "        .option(\"relationship.target.save.mode\", \"Match\")\n",
    "        .option(\"relationship.target.node.keys\", \"to_station:name\")\n",
    "        .save()\n",
    "    )\n",
    "    \n",
    "    # Create reverse relationships for bidirectional connections\n",
    "    (\n",
    "        line_data\n",
    "        .select(\n",
    "            F.col(\"to_station\").alias(\"from_station\"),\n",
    "            F.col(\"from_station\").alias(\"to_station\")\n",
    "        )\n",
    "        .write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .mode(\"Append\")\n",
    "        .option(\"relationship\", rel_type)\n",
    "        .option(\"relationship.save.strategy\", \"keys\")\n",
    "        .option(\"relationship.source.labels\", \":Station\")\n",
    "        .option(\"relationship.source.save.mode\", \"Match\")\n",
    "        .option(\"relationship.source.node.keys\", \"from_station:name\")\n",
    "        .option(\"relationship.target.labels\", \":Station\")\n",
    "        .option(\"relationship.target.save.mode\", \"Match\")\n",
    "        .option(\"relationship.target.node.keys\", \"to_station:name\")\n",
    "        .save()\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Created {connection_count} bidirectional connections for {line} line (:{rel_type})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✅ ALL {len(tube_lines_list)} TUBE LINES LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Final Validation\n",
    "\n",
    "Validate the complete London Transport Network graph with all tube lines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Complete Graph Validation\n",
    "\n",
    "Run comprehensive validation queries across all tube lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Connection errors:**\n",
    "- Verify Neo4j is running\n",
    "- Check firewall/network settings\n",
    "- Confirm credentials in Databricks Secrets\n",
    "- Ensure cluster access mode is \"Dedicated\"\n",
    "\n",
    "**File not found errors:**\n",
    "- Verify Unity Catalog volume path\n",
    "- Confirm CSV files are uploaded\n",
    "- Check catalog/schema names\n",
    "\n",
    "**Relationship creation issues:**\n",
    "- Ensure Station nodes exist before creating relationships\n",
    "- Verify station names match exactly between CSV files\n",
    "- Check for case sensitivity in station names\n",
    "\n",
    "**Library issues:**\n",
    "- Verify Neo4j Spark Connector is installed (Maven)\n",
    "- Verify Neo4j Python Driver is installed (PyPI)\n",
    "- Restart cluster if libraries were just installed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To extend this implementation:\n",
    "\n",
    "1. **Add more tube lines** - Modify Step 10 to process all tube lines instead of just Bakerloo\n",
    "2. **Create dynamic relationship types** - Use the tube line name to create different relationship types (e.g., :CENTRAL, :DISTRICT)\n",
    "3. **Add relationship properties** - Include additional metadata on relationships\n",
    "4. **Optimize performance** - Batch process large relationship sets\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Connection errors:**\n",
    "- Verify Neo4j is running\n",
    "- Check firewall/network settings\n",
    "- Confirm credentials in Databricks Secrets\n",
    "\n",
    "**File not found errors:**\n",
    "- Verify Unity Catalog volume path\n",
    "- Confirm CSV files are uploaded\n",
    "- Check catalog/schema names\n",
    "\n",
    "**Relationship creation issues:**\n",
    "- Ensure Station nodes exist before creating relationships\n",
    "- Verify station names match exactly between CSV files\n",
    "- Check for case sensitivity in station names\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
